{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#  Comparison of Segmentation Methods on X-ray Microtomography Data\n",
        "\n",
        "This notebook evaluates and compares three segmentation approaches applied to X-ray Microtomography (Micro-CT) images:\n",
        "\n",
        "1. **Pre-trained U-Net** – A convolutional encoder–decoder widely used in biomedical segmentation.  \n",
        "2. **Vision Transformer (ViT)** from *timm* – A transformer-based model adapted for image segmentation.  \n",
        "3. **Cross-Teaching Ensemble** – A hybrid method where U-Net and ViT exchange pseudo-labels during training and produce a combined prediction.\n",
        "\n",
        "The goal is to benchmark these models on the same dataset, assess their segmentation quality using metrics such as Dice score and IoU, and visualize qualitative differences through sample predictions. This provides insight into how classical CNN-based, transformer-based, and ensemble methods perform on Micro-CT segmentation tasks.\n"
      ],
      "metadata": {
        "id": "HX6SfwP9N2VH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports"
      ],
      "metadata": {
        "id": "wYyxqlTNNvK-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "rsieqNyMLyfP",
        "outputId": "6d8e861d-f805-4e8f-9f63-92cfbaf461c2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cpu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "from glob import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "import timm\n",
        "from skimage.filters import threshold_otsu\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import jaccard_score\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load and Visualize Data"
      ],
      "metadata": {
        "id": "eVDQyKIqPEPv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================================\n",
        "#   Helper: Read image as grayscale float32 numpy array\n",
        "# ============================================================\n",
        "def load_image(path):\n",
        "    img = Image.open(path).convert(\"L\")\n",
        "    return np.array(img).astype(np.float32)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "#   Evaluation-only Dataset for X-ray Microtomography Segmentation\n",
        "# ============================================================\n",
        "class XrayMicroCTDataset(Dataset):\n",
        "    \"\"\"\n",
        "    (This is reformatted from the Unet_TransferLearn.py script)\n",
        "    Evaluation-only dataset:\n",
        "        ✓ No augmentations\n",
        "        ✓ No random operations\n",
        "        ✓ Strictly deterministic\n",
        "        ✓ Otsu thresholding for masks (optional)\n",
        "        ✓ Fixed normalization (min-max or z-score)\n",
        "        ✓ Returns image + mask as torch tensors\n",
        "\n",
        "    Suitable for inference, visualization, and model comparison.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        image_dir,\n",
        "        mask_dir=None,\n",
        "        resize_to=512,\n",
        "        normalize=\"minmax\",   # \"minmax\", \"zscore\", or None\n",
        "        use_otsu=True,\n",
        "    ):\n",
        "        self.image_paths = sorted(glob(os.path.join(image_dir, \"*\")))\n",
        "        self.mask_paths = sorted(glob(os.path.join(mask_dir, \"*\"))) if mask_dir else None\n",
        "\n",
        "        self.resize_to = resize_to\n",
        "        self.normalize = normalize\n",
        "        self.use_otsu = use_otsu\n",
        "\n",
        "        assert len(self.image_paths) > 0, \"No images found!\"\n",
        "        if self.mask_paths:\n",
        "            assert len(self.image_paths) == len(self.mask_paths), \\\n",
        "                \"Image and mask count mismatch.\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def _resize(self, arr):\n",
        "        \"\"\"Resize using PIL with correct interpolation.\"\"\"\n",
        "        img = Image.fromarray(arr.astype(np.float32))\n",
        "        img = img.resize((self.resize_to, self.resize_to), Image.BILINEAR)\n",
        "        return np.array(img).astype(np.float32)\n",
        "\n",
        "    def _normalize(self, arr):\n",
        "        if self.normalize is None:\n",
        "            return arr\n",
        "\n",
        "        if self.normalize == \"minmax\":\n",
        "            mn, mx = arr.min(), arr.max()\n",
        "            if mx > mn:\n",
        "                return (arr - mn) / (mx - mn)\n",
        "            else:\n",
        "                return arr * 0  # degenerate case\n",
        "\n",
        "        if self.normalize == \"zscore\":\n",
        "            mean = arr.mean()\n",
        "            std = arr.std() + 1e-6\n",
        "            return (arr - mean) / std\n",
        "\n",
        "        return arr\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # -----------------------------\n",
        "        # Load image\n",
        "        # -----------------------------\n",
        "        img = load_image(self.image_paths[idx])\n",
        "        img = self._resize(img)\n",
        "        img = self._normalize(img)\n",
        "\n",
        "        # Convert to tensor: (1, H, W)\n",
        "        img_t = torch.from_numpy(img).unsqueeze(0)\n",
        "\n",
        "        # -----------------------------\n",
        "        # Load mask (if exists)\n",
        "        # -----------------------------\n",
        "        if self.mask_paths is not None:\n",
        "            mask = load_image(self.mask_paths[idx])\n",
        "\n",
        "            mask = mask.astype(np.float32)\n",
        "            mask = self._resize(mask)\n",
        "\n",
        "            # Optional: Otsu threshold for clean segmentation targets\n",
        "            if self.use_otsu:\n",
        "                t = threshold_otsu(mask)\n",
        "                mask = (mask > t).astype(np.float32)\n",
        "            else:\n",
        "                mask = (mask > 0.5).astype(np.float32)\n",
        "\n",
        "            mask_t = torch.from_numpy(mask).unsqueeze(0)\n",
        "        else:\n",
        "            mask_t = None\n",
        "\n",
        "        return img_t, mask_t, os.path.basename(self.image_paths[idx])"
      ],
      "metadata": {
        "id": "0UFjMJZfOdA3"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "#   Visualization Helpers\n",
        "# ============================================================\n",
        "\n",
        "def show_image(img_tensor, title=\"Image\"):\n",
        "    img = img_tensor.squeeze().cpu().numpy()\n",
        "    plt.figure(figsize=(4,4))\n",
        "    plt.imshow(img, cmap=\"gray\")\n",
        "    plt.title(title)\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def show_image_and_mask(img, mask, filename=\"\"):\n",
        "    img = img.squeeze().cpu().numpy()\n",
        "    mask = mask.squeeze().cpu().numpy()\n",
        "\n",
        "    plt.figure(figsize=(10,4))\n",
        "    plt.suptitle(filename)\n",
        "\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.imshow(img, cmap=\"gray\")\n",
        "    plt.title(\"Image\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.imshow(mask, cmap=\"gray\")\n",
        "    plt.title(\"Mask (binary)\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def compare_predictions(img, mask, unet_pred, vit_pred, ensemble_pred, filename=\"\"):\n",
        "    \"\"\"Side-by-side comparison for evaluation notebook.\"\"\"\n",
        "    img  = img.squeeze().cpu().numpy()\n",
        "    mask = mask.squeeze().cpu().numpy()\n",
        "    u    = unet_pred.squeeze().cpu().numpy()\n",
        "    v    = vit_pred.squeeze().cpu().numpy()\n",
        "    e    = ensemble_pred.squeeze().cpu().numpy()\n",
        "\n",
        "    plt.figure(figsize=(16,6))\n",
        "    plt.suptitle(f\"Segmentation Comparison – {filename}\", fontsize=14)\n",
        "\n",
        "    titles = [\n",
        "        \"Input Image\",\n",
        "        \"Ground Truth Mask\",\n",
        "        \"U-Net Prediction\",\n",
        "        \"ViT Prediction\",\n",
        "        \"Ensemble Output\",\n",
        "    ]\n",
        "    arrays = [img, mask, u, v, e]\n",
        "\n",
        "    for i, (arr, t) in enumerate(zip(arrays, titles)):\n",
        "        plt.subplot(1, 5, i+1)\n",
        "        plt.imshow(arr, cmap=\"gray\")\n",
        "        plt.title(t)\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "7xMZNybJa_bR"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "IMAGE_DIR = \"/path/to/labeled/images\"           # ⚠️ UPDATE ME\n",
        "MASK_DIR  = \"/path/to/labeled/masks\"            # ⚠️ UPDATE ME\n",
        "\n",
        "dataset = XrayMicroCTDataset(\n",
        "    image_dir=IMAGE_DIR,\n",
        "    mask_dir=MASK_DIR,\n",
        "    resize_to=512,\n",
        "    normalize=\"minmax\",\n",
        "    use_otsu=True\n",
        ")\n",
        "\n",
        "# Create dataset\n",
        "dataset = SegmentationDataset(image_files, mask_files)\n",
        "sample_img, sample_mask = dataset[0]\n",
        "sample_img = sample_img.unsqueeze(0).to(DEVICE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "gSjG7d57WH5p",
        "outputId": "4072c608-223c-4f4c-dbe3-9c5804f31581"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "No images found!",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4291620507.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mMASK_DIR\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0;34m\"/path/to/labeled/masks\"\u001b[0m            \u001b[0;31m# ⚠️ UPDATE ME\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m dataset = XrayMicroCTDataset(\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mimage_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mIMAGE_DIR\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mmask_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMASK_DIR\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2903445953.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, image_dir, mask_dir, resize_to, normalize, use_otsu)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_otsu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_otsu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_paths\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"No images found!\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask_paths\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_paths\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: No images found!"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img, mask, fname = dataset[0]\n",
        "show_image_and_mask(img, mask, fname)"
      ],
      "metadata": {
        "id": "sDGFJuC8b4E4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load in Models and Model Architectures"
      ],
      "metadata": {
        "id": "guHupSFnPIjn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Vision Transformer Segmentation Head\n",
        "# -----------------------------\n",
        "class ViTSegmentationHead(nn.Module):\n",
        "    def __init__(self, embed_dim=1024, num_classes=1):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "        self.conv = nn.Conv2d(embed_dim, num_classes, kernel_size=1)\n",
        "\n",
        "    def forward(self, features):\n",
        "        normed = self.norm(features)\n",
        "        seg_map = self.conv(normed)\n",
        "        seg_map = torch.sigmoid(seg_map)\n",
        "        return seg_map\n",
        "\n",
        "# -----------------------------\n",
        "# Vision Transformer Segmentation Model\n",
        "# -----------------------------\n",
        "class ViTSegmentation(nn.Module):\n",
        "    def __init__(self, num_classes=1, img_size=224):\n",
        "        super().__init__()\n",
        "\n",
        "        self.vit = create_model(\n",
        "            'vit_large_patch16_224',\n",
        "            pretrained=True,\n",
        "            img_size=img_size,\n",
        "            in_chans=1,\n",
        "            num_classes=0\n",
        "        )\n",
        "        for param in self.vit.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        embed_dim = self.vit.embed_dim\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(embed_dim, 512, kernel_size=2, stride=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(256, embed_dim, kernel_size=2, stride=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.seg_head = ViTSegmentationHead(embed_dim=embed_dim, num_classes=num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        vit_out = self.vit.forward_features(x)\n",
        "        b, hw, embed = vit_out.shape\n",
        "        side = int(hw ** 0.5)\n",
        "        feat_2d = vit_out.transpose(1, 2).reshape(b, embed, side, side)\n",
        "        upsampled = self.decoder(feat_2d)\n",
        "        seg_map = self.seg_head(upsampled)\n",
        "        return seg_map\n",
        "\n",
        "    def load_vit_weights(self, path):\n",
        "        state_dict = np.load(path)['params']\n",
        "        self.load_state_dict(state_dict, strict=False)\n",
        "        print(f\"Loaded custom ViT weights from {path}\")\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# U-Net Loader Function\n",
        "# -----------------------------\n",
        "def load_unet_model(unet_path, device=\"cuda\"):\n",
        "    sys.path.append(\"..\")  # ⚠️ MAY NEED UPDATE\n",
        "    from unet_pytorch import create_unet_for_porosity  # ⚠️ CHECK PATH\n",
        "\n",
        "    model = create_unet_for_porosity().to(device)\n",
        "    checkpoint = torch.load(unet_path, map_location=device)\n",
        "\n",
        "    if 'state_dict' in checkpoint:\n",
        "        model.load_state_dict(checkpoint['state_dict'])\n",
        "    else:\n",
        "        model.load_state_dict(checkpoint)\n",
        "\n",
        "    model.eval()\n",
        "    print(f\"Loaded U-Net model from {unet_path}\")\n",
        "    return model\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Cross Teaching Trainer (for ensemble inference)\n",
        "# -----------------------------\n",
        "class CrossTeachingTrainer:\n",
        "    def __init__(self, unet, vit, device=\"cuda\"):\n",
        "        self.unet = unet.eval()\n",
        "        self.vit = vit.eval()\n",
        "        self.device = device\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def ensemble_predict(self, images):\n",
        "        images = images.to(self.device)\n",
        "\n",
        "        unet_in = F.interpolate(images, size=(512, 512), mode=\"bilinear\", align_corners=False)\n",
        "        vit_in = F.interpolate(images, size=(224, 224), mode=\"bilinear\", align_corners=False)\n",
        "\n",
        "        unet_pred = self.unet(unet_in)\n",
        "        vit_pred = self.vit(vit_in)\n",
        "\n",
        "        unet_pred_up = F.interpolate(unet_pred, size=images.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
        "        vit_pred_up = F.interpolate(vit_pred, size=images.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
        "\n",
        "        return (unet_pred_up + vit_pred_up) / 2\n",
        "\n",
        "#Metric for segmentation performance\n",
        "def dice_score(pred, target, eps=1e-6):\n",
        "    pred_bin = (pred > 0.5).float()\n",
        "    target_bin = (target > 0.5).float()\n",
        "    intersection = (pred_bin * target_bin).sum()\n",
        "    return (2 * intersection) / (pred_bin.sum() + target_bin.sum() + eps)"
      ],
      "metadata": {
        "id": "pWeXFq4MSmYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "UNET_PATH = \"/path/to/unet_checkpoint.pth\"     # ⚠️ UPDATE ME\n",
        "VIT_PRETRAIN = \"/path/to/vit_weights.npz\"       # ⚠️ UPDATE ME\n",
        "XUNET_PATH = \"/path/to/Xunet_checkpoint.pth\"     # ⚠️ UPDATE ME\n",
        "XVIT_PRETRAIN = \"/path/to/vit_weights.npz\"       #⚠️ UPDATE ME\n",
        "# Load Models\n",
        "unet = load_unet_model(UNET_PATH, device=DEVICE)\n",
        "vit = ViTSegmentation().to(DEVICE)\n",
        "\n",
        "Xunet = load_unet_model(XUNET_PATH, device=DEVICE)\n",
        "Xvit = ViTSegmentation().to(DEVICE)\n",
        "\n",
        "# Optional: load custom ViT weights                   <---- do this\n",
        "vit.load_vit_weights(VIT_PRETRAIN)\n",
        "Xvit.load_vit_weights(XVIT_PRETRAIN)\n",
        "\n",
        "\n",
        "# Ensemble\n",
        "ensemble = CrossTeachingTrainer(unet, vit, device=DEVICE)"
      ],
      "metadata": {
        "id": "_ziTE18SWMnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inference Helpers"
      ],
      "metadata": {
        "id": "obIj8hxEUd4c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def predict_unet(unet, image_tensor, device=\"cuda\"):\n",
        "    image_resized = F.interpolate(image_tensor, size=(512, 512),\n",
        "                                  mode=\"bilinear\", align_corners=False)\n",
        "    pred = unet(image_resized)\n",
        "    pred = F.interpolate(pred, size=image_tensor.shape[-2:],\n",
        "                         mode=\"bilinear\", align_corners=False)\n",
        "    return pred\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def predict_vit(vit, image_tensor, device=\"cuda\"):\n",
        "    image_resized = F.interpolate(image_tensor, size=(224, 224),\n",
        "                                  mode=\"bilinear\", align_corners=False)\n",
        "    pred = vit(image_resized)\n",
        "    pred = F.interpolate(pred, size=image_tensor.shape[-2:],\n",
        "                         mode=\"bilinear\", align_corners=False)\n",
        "    return pred"
      ],
      "metadata": {
        "id": "mu4q8IrAUfOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize"
      ],
      "metadata": {
        "id": "W2DvABy_UirN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_predictions(image, mask, unet_pred, vit_pred, ensemble_pred):\n",
        "    image = image.squeeze().cpu().numpy()\n",
        "    mask = mask.squeeze().cpu().numpy()\n",
        "    up = unet_pred.squeeze().cpu().numpy()\n",
        "    vp = vit_pred.squeeze().cpu().numpy()\n",
        "    ep = ensemble_pred.squeeze().cpu().numpy()\n",
        "\n",
        "    plt.figure(figsize=(15, 8))\n",
        "\n",
        "    titles = [\"Image\", \"Ground Truth\", \"U-Net\", \"ViT\", \"Ensemble\"]\n",
        "    imgs = [image, mask, up, vp, ep]\n",
        "\n",
        "    for i, (t, im) in enumerate(zip(titles, imgs), 1):\n",
        "        plt.subplot(2, 3, i)\n",
        "        plt.imshow(im, cmap=\"gray\")\n",
        "        plt.title(t)\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "idx = 0\n",
        "image, mask = dataset[idx]\n",
        "\n",
        "image_tensor = image.unsqueeze(0).to(DEVICE)   # shape: (1, 1, H, W)\n",
        "mask_tensor  = mask.unsqueeze(0).to(DEVICE)\n",
        "\n",
        "# Get model predictions\n",
        "unet_pred = predict_unet(unet, image_tensor, device=DEVICE)\n",
        "vit_pred = predict_vit(vit, image_tensor, device=DEVICE)\n",
        "ensemble_pred = ensemble.ensemble_predict(image_tensor)\n",
        "\n",
        "# Visualize\n",
        "visualize_predictions(\n",
        "    image_tensor,\n",
        "    mask_tensor,\n",
        "    unet_pred,\n",
        "    vit_pred,\n",
        "    ensemble_pred\n",
        ")"
      ],
      "metadata": {
        "id": "nVyl3X4bVyle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_unet = predict_unet(unet, sample_img)\n",
        "pred_vit = predict_vit(vit, sample_img)\n",
        "pred_ensemble = ensemble.ensemble_predict(sample_img)"
      ],
      "metadata": {
        "id": "Iaw1mpztV1oS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluations"
      ],
      "metadata": {
        "id": "J4a_oPzVi8yh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Metric utilities\n",
        "# -----------------------------\n",
        "\n",
        "def dice_score(pred, target):\n",
        "    \"\"\"Compute Dice score for binary segmentation.\"\"\"\n",
        "    pred = pred.flatten()\n",
        "    target = target.flatten()\n",
        "\n",
        "    intersection = (pred * target).sum()\n",
        "    return (2 * intersection) / (pred.sum() + target.sum() + 1e-6)\n",
        "\n",
        "\n",
        "def iou_score(pred, target):\n",
        "    \"\"\"Compute Intersection-over-Union.\"\"\"\n",
        "    pred = pred.flatten()\n",
        "    target = target.flatten()\n",
        "\n",
        "    intersection = (pred * target).sum()\n",
        "    union = pred.sum() + target.sum() - intersection\n",
        "    return intersection / (union + 1e-6)\n",
        "\n",
        "\n",
        "def precision_recall(pred, target):\n",
        "    \"\"\"Compute precision and recall using TP, FP, FN.\"\"\"\n",
        "    pred = pred.flatten()\n",
        "    target = target.flatten()\n",
        "\n",
        "    tp = (pred * target).sum()\n",
        "    fp = (pred * (1 - target)).sum()\n",
        "    fn = ((1 - pred) * target).sum()\n",
        "\n",
        "    precision = tp / (tp + fp + 1e-6)\n",
        "    recall = tp / (tp + fn + 1e-6)\n",
        "\n",
        "    return precision, recall\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Evaluation function\n",
        "# -----------------------------\n",
        "\n",
        "def evaluate_models(dataset, unet, vit, ensemble, device=\"cuda\"):\n",
        "    \"\"\"\n",
        "    Evaluate U-Net, ViT, and Ensemble segmentation models using\n",
        "    consistent metrics (Dice, IoU, Precision, Recall)\n",
        "    on the provided XrayMicroCTDataset.\n",
        "\n",
        "    Returns and prints a clean summary table.\n",
        "    \"\"\"\n",
        "\n",
        "    # Metric accumulators\n",
        "    metrics = {\n",
        "        \"U-Net\": {\"dice\": [], \"iou\": [], \"prec\": [], \"rec\": []},\n",
        "        \"ViT\":   {\"dice\": [], \"iou\": [], \"prec\": [], \"rec\": []},\n",
        "        \"Ensemble\": {\"dice\": [], \"iou\": [], \"prec\": [], \"rec\": []},\n",
        "    }\n",
        "\n",
        "    unet.eval()\n",
        "    vit.eval()\n",
        "    ensemble.eval()\n",
        "\n",
        "    # -------------------------\n",
        "    # Main evaluation loop\n",
        "    # -------------------------\n",
        "    with torch.no_grad():\n",
        "        for img_t, mask_t, fname in dataset:\n",
        "\n",
        "            # Move to device\n",
        "            img = img_t.to(device).unsqueeze(0)    # add batch dimension\n",
        "            mask = mask_t.to(device).unsqueeze(0)  # [1,1,H,W]\n",
        "\n",
        "            # Ground truth (binary)\n",
        "            target = (mask > 0.5).float()\n",
        "\n",
        "            # ---- Predictions ----\n",
        "            pred_unet = (predict_unet(unet, img) > 0.5).float()\n",
        "            pred_vit = (predict_vit(vit, img) > 0.5).float()\n",
        "            pred_ens = (ensemble.ensemble_predict(img) > 0.5).float()\n",
        "\n",
        "            # ---- Metrics ----\n",
        "            for name, pred in [\n",
        "                (\"U-Net\", pred_unet),\n",
        "                (\"ViT\", pred_vit),\n",
        "                (\"Ensemble\", pred_ens),\n",
        "            ]:\n",
        "                d = dice_score(pred.cpu().numpy(), target.cpu().numpy())\n",
        "                i = iou_score(pred.cpu().numpy(), target.cpu().numpy())\n",
        "                p, r = precision_recall(pred.cpu().numpy(), target.cpu().numpy())\n",
        "\n",
        "                metrics[name][\"dice\"].append(d)\n",
        "                metrics[name][\"iou\"].append(i)\n",
        "                metrics[name][\"prec\"].append(p)\n",
        "                metrics[name][\"rec\"].append(r)\n",
        "\n",
        "    # -------------------------\n",
        "    # Create summary table\n",
        "    # -------------------------\n",
        "    summary = pd.DataFrame({\n",
        "        \"Model\": [\"U-Net\", \"ViT\", \"Ensemble\"],\n",
        "        \"Dice\": [np.mean(metrics[m][\"dice\"]) for m in metrics],\n",
        "        \"IoU\": [np.mean(metrics[m][\"iou\"]) for m in metrics],\n",
        "        \"Precision\": [np.mean(metrics[m][\"prec\"]) for m in metrics],\n",
        "        \"Recall\": [np.mean(metrics[m][\"rec\"]) for m in metrics],\n",
        "    })\n",
        "\n",
        "    print(\"\\n=== Segmentation Performance Summary ===\")\n",
        "    display(summary.style.set_caption(\"Model Comparison on X-ray Micro-CT Segmentation\"))\n",
        "\n",
        "    return summary\n",
        "\n"
      ],
      "metadata": {
        "id": "Bu7zxemhi8GM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary = evaluate_models(\n",
        "    eval_loader,\n",
        "    unet_model,\n",
        "    vit_model,\n",
        "    ensemble_model,\n",
        "    device=device\n",
        ")"
      ],
      "metadata": {
        "id": "F5u1i05KjKqg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}